# 1. Single Layer Perceptron

# 1.1 퍼셉트론 신경망 구조

- 단층 퍼셉트론 경우 입력벡터(input layer), 출력벡터(output layer)으로 구성되며,

    각 입력벡터(x1~xi)는 가중치 벡터와 편향값 (bias, xo*wo)을 이용하여 결합함수(시그마), 활성함수 (임계함수)를 거쳐 출력 벡터를 뱉는 구조

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%201.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%201.png)

- 식으로 보면 z = w1*x1 + w2*x2 + ... wn * xn 까지가 결합 함수 (회귀식과 동일)
- 결합 함수를 Activation Function(임계함수)을 거쳐 출력한다.
- (계단식 함수라면) z(net) 값이 임계치를 넘냐 안넘냐 에 따라 0,1 호출
*Q : 바이너리 문제가 아닌 회귀 문제라면 활성화 함수는 어떤게 들어가나??*

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%202.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%202.png)

# 1.2 텐서 연산과 미니배치의 활용

- **텐서란?** 
텐서를 엄밀하게 정의하기란 쉽지는 않지만 딥러닝에서는 다차원 숫자 배열이라고 이해해도 무리 없음
e.g. 0차원 스칼라, 1차원 벡터, 2차원 행렬 모두 텐서 3차원도 물론 텐서
- **텐서가 중요한 이유는?**
같은 문제라도 텐서를 이용해 처리하는 편이 프로그램 최적화 및 처리 속도도 훨씬 빠르다고 한다.
인터프리터 자체가 반복문 보다 텐서 연산을 효율적으로 처리 (GPU 환경에서는 더더욱!)
Q : 반복문보다 효율적으로 처리하는건 알겠는데 텐서의 연산 방식이 멀까??
→ 벡터 내적 연산으로 x*w 단번의 계산하는 편이 간단
(같은 위치에 곱의 합)

- **미니배치란?**
일반적으로 딥러닝에서는 여러 데이터를 한꺼번에 처리하는데 이를 미니배치라고 한다.
(그냥 n수의 데이터를 신경망 계산을 하는 것을 미니 배치라고 하는 듯 ?)
이미지 같은 경우 데이터가 커서 배치 단위로 나누어서 돌린다. (2^n 단위)

- **단층 퍼셉트론 (SLP)의 동작 방식 (입력층 → 출력층)**  
아래 그림을 보면 
(a) 는 퍼셉트론 하나의 동작 방식 —> 1개의 퍼셉트론 1 동그라미 x1*w1+x2*w..x4*w4 + b = y1
(b) 는 SLP를 구성하는 일련의 퍼셉트론(여러 동그라미) 즉 1개의 o1이 데이터를 1개 처리하는 방식 
가중치 벡터는 퍼셉트론의 수만큼 가중치 행렬 W가 되고 출력 벡터 (yi 값)
yi = x1*w1i + .. + xn*win + bi로 계산되고 한꺼번에 y = xW+b로 표현 (행렬 연산 상태로 한번에 계산) 
****(c) 그림은 (b) 그림을 n개의 데이터를 처리

여기서 학습데이터를 n개만큼 1번 처리하면 1**epoch**(에폭)
반복 수에는 고려되지 않으나 신경망 구조나 학습 결과에 영향을 미치는 고려요인은 **하이퍼파라미터**라고 한다
(e.g. 러닝레이트)

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%203.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%203.png)

## 1.2.1 SLP 예제

- 임계값의 조정
아래 그림의 퍼셉트론의 구조를 보면 입력층은 가중치와 함께 결합 함수를 거쳐 활성함수(임계함수를) 거쳐서 출력층이 0 or 1로 나오는 방식인데 이 때 활성함수의 임계값을 넘었냐 못넘었냐로 결정된다.

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%204.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%204.png)

이 때 수학적으로 세타값을 넘었냐 못넘었냐로 보지 않고 값을 넘겨서 0을 넘었냐 못넘었냐로 보게 되면 (축의 이동) 세타값을 하나의 입력층과 가중치의 곱으로 보게되는데 보통 x0에 1을 줘서 w0=세타 값이 되게 된다

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%205.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%205.png)

- 학습
결국에는 퍼셉트론 학습이란 최상의 결과를 도출하는 연결강도 (세타값, 가중치)를 찾는 것으로 볼 수 있는데,
case updating 방식을 사용하게 된다. (배치 업데이팅도 있다고 하는데 이건 언제쓰는지 안 찾아봄)

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%206.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%206.png)

활성함수(임계함수)를 거치면 예측값을 얻을 수 있는데 이 예측 값과 실제 값의 오차와 lr, 입력값과의 곱으로 가중치를 업데이트한다. 

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%207.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%207.png)

- 예제
예제를 보면서 풀이 해보자 
Q : 실제 학습할 때도 초기 가중치가 주어지는지?

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%208.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%208.png)

이때 결합함수 값은 가중치 벡터와 입력 벡터의 내적으로 계산한다. 

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%209.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%209.png)

예제 활성함수는 계단식함수를 사용하였는데, 결합 함수 값이 -0.281 < 0 이므로 첫 번째  y 예측 값은 -1
오차 값 1 - (-1) = 2
가중치 증감률은 0.2(러닝레이트) * 2(오차) * xj(입력 값)

w0 증감률 0.2*2* 1(임계값 조정으로 인해서 x0를 1로 주었다.) = 0.4

w1 증감률 0.2 * 2.  0.2 = 0.08

w2 증감률 0.2 * 2 * 0.9 = 0.36

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2010.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2010.png)

첫 번째 x 입력 값으로 인해서 각 가중치 증감률을 구하였는데
이를 통해서 업데이트 된 가중치를 구할 수 있다. 

new w0 = old w0 + w0 증감률 = -0.3 +  0.4 = 0.1

new w1 = old w1 + w1 증감률 = 0.05 + 0.08 = 0.13

new w2 = old w2 + w2 증감률 = 0.01 +  0.36 = 0.37

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2011.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2011.png)

요딴식으로 계산해서 n수 만큼 돌면 1에폭이 끝난 것 

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2012.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2012.png)

# 1.3 신경망의 세 가지 기본 출력 유형과 회귀 분석

알고리즘은 세분화하면 회귀, 이진 분류, 선택 분류(multi class)

# 1.4 전복의 고리 수 추정 문제

skip

# 1.5 회귀 분석과 평균제곱오차(MSE) 손실 함수

회귀분석에서 실제값 y을 예측값이 얼마나 정확한지 판단하는 것을 MSE로 평가지표로 삼는다.

딥러닝에서는 값이 항상 0이상이며, 추정이 정확해질 수록 값이 작아지는 성질이 있으면서 미분도 가능한 평가지표를 정의한 후 이를 최소화하는 것을 목표로 학습을 수행한다. **이를 손실 함수(loss function), 비용 함수(cost function)이 라고한다.** 

# 1.6 경사하강법과 역전파

경사하강법 (gradient descent algorithm)은 함수의 기울기를 반복 계산하면서 이 기울기에 따라 함숫값이 낮아지는 방향으로 이동하는 기본적인 딥러닝 학습 알고리즘 (함수값이 낮아지는 값을 확인)

딥러닝은 기본적으로 가변 파라미터를 갖는 신경망 구조를 설정한 후  학습을 통해 파라미터(가중치, 편향) 값들을 조절하여 신경망이 원하는 동작을 수행하도록 만드는 인공지능 기법 

경사하강법은 순전파와 역전파 과정을 번갈아 수행하는 과정을 반복하면서 신경망 파라미터(가중치,편향)을 원하는 값으로 바꾸어 나간다.

**순전파**란 입력 데이터에 대해 신경망 구조를 따라가면서 현재의 파라미터값들을 이용해 손실 함숫값을 계산하는 과정을 말한다. 

**역전파**란 순전파의 계산 과정을 역순으로 거슬러가면서 손실 함숫값에 직간접적으로 영향을 미친 모든 성분에 대하여 손실 기울기를 계산하는 과정

Q 경사하강법으로 손실기울기가 최소화 되는 값을 구하는게 학습량?

에폭 vs 이터 

**iteration**

*The number of passes to complete one epoch.*

**batch size는 한 번의 batch마다 주는 데이터 샘플의 size. 여기서 batch(보통 mini-batch라고 표현)는 나눠진 데이터 셋을 뜻하며 iteration는 epoch를 나누어서 실행하는 횟수라고 생각하면 됨.**

**▶ 메모리의 한계와 속도 저하 때문에 대부분의 경우에는 한 번의 epoch에서 모든 데이터를 한꺼번에 집어넣을 수는 없습니다. 그래서 데이터를 나누어서 주게 되는데 이때 몇 번 나누어서 주는가를 iteration, 각 iteration마다 주는 데이터 사이즈를 batch size라고 합니다.**



# 1.7 편미분과 손실 기울기의 계산

![1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2019.png](1%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%8E%E1%85%B3%E1%86%BC%20%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB(SLP)%207471a49c630640f29ab07ccccf1f56d8/Untitled%2019.png)
