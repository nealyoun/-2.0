# MLP (Chapter 4)

Assign: Anonymous
Due Date: September 30, 2021
Status: In Progress

## 4.1 다층 퍼셉트론 신경망

- 다층 퍼셉트론 신경망: 복수의 퍼셉트론 계층을 순서를 두고 배치한 것
    - 단층 퍼셉트론(엄밀히 0개의 은닉 계층을 보유한 MLP)에서 하나 이상의 은닉 계층을 갖는 구조
    - 입력 벡터 → 중간 표현 → 출력 벡터

![Untitled](MLP%20(Chapter%204)%206c05795f5c3a4c3583554852107ad0d4/Untitled.png)

하나의 계층 안에 속한 퍼셉트론들은 동일한 입력을 공유 및및 각각의 출력 성분을 생성하지만 서로 어떠한 연결도 없어 영향을 주고받을 수 없음

but, 인접한 계층끼리는 앞 계층의 출력이 뒤 계층의 모든 퍼셉트론에 공통으로 입력된다. 따라서 MLP의 인접 계층끼리는 방향성을 갖는 완전 연결 방식으로 연결된다

### Output Layer

- SLP에서 유일하게 존재했던 계층
- 문제 성격에 알맞은 출력 벡터 생성을 하기 위해 존재

### Hidden Layer

- Output layer 앞단에 배치되어 입력 벡터들을 전처리
- 출력에 직접 드러나지 않는 계층
- 은닉 계층이 생성하는 중간 표현 = 은닉 벡터

## 4.2 은닉 계층의 수와 폭

출력 계층: MLP 에서 최종 단계에 배치된 계층은 신경망에 주어진 원래의 임무에 따라 알맞는 형태의 출력 벡터를 생성하는 것이 목적 

→ 퍼셉트론의 수, 출력 벡터의 크기는 정해져 있다

은닉 계층: 출력 계층과는 다르게 위와 같은 제약이 없다. 따라서 은닉 계층의 수, 은닉의 폭은 신경망 설계자가 자유럽게 설정한다.

![Untitled](MLP%20(Chapter%204)%206c05795f5c3a4c3583554852107ad0d4/Untitled.png)

- hidden layer width: 해당 계층이 갖는 퍼셉트론의 수이자 은닉 벡터의 크기
- input vector: 4
- output vector: 4
- hidden layer: 3 nodes each

위 그림에서 퍼셉트론 수는 출력 계층 4, 두 은닉 계측에서 각각 3개로 합이 10

한 계층의 퍼셉트론들은 각각 해당 계층에 대한 입력 벡터 크기만큼의 weight와 bias를 가짐

- 입력 m개에 연결된 퍼셉트론 n개를 갖는 한층의 weight parameter 수 = m x n
- 입력 m개에 연결된 퍼셉트론 n개를 갖는 한층의 bias parameter 수 = n
- 총 parameter = m x n + n

은닉 계층의 수와 은닉 계층의 폭은 신경망의 품질을 결정짓는 중요한 요인이지만 무조건 그 수를 늘린다고 품질이 좋아지는 것은 아니다

parameter의 수가 늘어나는 만큼 더 많은 학습 데이터가 필요하며, 충분한 양의 양질의 데이터가 준비되지 않으면 MLP의 성능을 떨어뜨릴 수 있다.

## 4.3 비선형 활성화 함수

비선형 활성화 함수: 은닉 계층에서의 선형 연산 결과 뒷단에 적용되어 퍼셉트론의 출력을 변형시키기 위해 추가한 장치이며 입력의 일차 함수 표현을 넘어서는 복잡한 퍼렙트론 출력을 만들 수 있다

- 비선형 함수는 선형 연산 결과를 비선형으로 변환 시켜주는 역할
- 활성화 함수는 은닉 계층 뒷단에 추가된 퍼셉트론의 출력을 변형 시켜주는 역할
- 비선형 활성화 함수를 갖춘 은닉 계층을 바탕으로 단 두 계층의 MLP 구조만으로 어떤 수학적 함수이든 원하는 오차 수준 이내로 근사하게 동작하도록 만들 수 있음이 수학적으로 증명됨

### 출력 계층

- 비선형 활성화 함수를 두지 않는다
    - sigmoid, softmax와 같은 함수를 이용한다
    - 출력 계층의 경우 출력 유형에 따라 비선형 함수를 이용하는 방법이 달라지기 때문에 SLP에서는 이들을 퍼셉트론 내의 구성 요소로 간주하지 않는다
    - ex) softmax 함수는 출력 여러 개를 묶어 함께 처리하기 때문에 독립된 구조인 개별 퍼셉트론 안에 해당 함수를 나누어 넣을 수 없다
    - 출력 벡터를 구성하는 각 정보의 특성에 맞게 별도의 비선형 처리를 하는 복합 출력을 해야하기 때문

### 은닉 계층

- 비선형 활성화 함수는 필수적 구성 요소
    - 비선형 활성화 함수 없이는 독립된 계층으로서 무의미
    - 선형 처리는 아무리 여러 단계를 반복해도 하나의 선형 처리로 표현 될수있는 수학적 원리
    - 다양한 종류의 비선형 함수를 사용할 수 있지만 ReLU를 가장 널리 이용

은닉 계층이 하나일 때 문제 내용이 복잡하면 은닉 계층의 node의 수가 기하급수적으로 늘어난다.

하지만 다양한 실험에 따르면 node 수가 많은 SLP 보다 node 수가 적은 MLP 구조 신경망의 성능이 우수한 경우가 많다.

- 많은 node를 갖는 신경망 → layer는 많아도 node를 적게 가져가는 신경망을 추구
- deep learning의 "deep"이라는 표현이 사용되는 이유에 해당

## 4.4 ReLU

**Rectified Linear Unit** 함수는 음수 입력을 걸러내 0으로 만드는 간단한 기능을 제공

즉, x > 0 일 때는 y = x, x ≤ 0일 때는 y = 0을로 정의된다

![Untitled](MLP%20(Chapter%204)%206c05795f5c3a4c3583554852107ad0d4/Untitled%201.png)

- sigmoid, softmax 도 비선형 함수지만 지수 연산이 포함되어 복잡한 계산 과정으로 처리 부담이 크다. (softmax 함수는 벡터 원소들을 묶어 처리하기 때문에 은닉 계층 출력 처리에 부적절)
- 반면 ReLU 함수는 위와 같이 꺽인 선 모양이기 때문에 비선형 함수이기에 비선형 특성을 추가할 수 있으며 빠른 계산 처리가 가능하다

### ReLU 미분

- x > 0 일 때 y' = 1, x < 0 일 때 y' = 0
- x = 0 일 때는 미분 불가능
    - 따라서 x = 0 일 때 y' = 0 으로 정의해도 무방
- y > 0 일 때 y' = 1, y = 0 일 때 y' = 0
- y < 0 일 때 y' = -1로 정의 가능
    - 이는 ReLU의 출력이 음수일 수 없기 때문에 y가 음수인 경우를 제외할 수 있기 때문이다

## 4.5 XOR & 비선형 활성화 함수
