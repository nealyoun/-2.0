# CNN (Chapter 7.5~)

Assign: Anonymous
Due Date: November 7, 2021
Reference: https://je-d.tistory.com/entry/%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9DCNN
Status: In Progress

## 7.5 채널의 도입과 커널의 확장

Convolution 연산에서 커널은(필터) input 영역의 특정 패턴이나 특징에 민감하게 반응하는 방향으로 학습되는 경향이 있음

→ convolution layer에서 출력되는 **픽셀 이미지**를 **Feature Map (특징맵)** 이라 한다

이미지는 다양한 패턴과 특징을 보유 즉, 한 가지 feature map으로 대체 불가 = 커널 또한 하나로 충분하지 않음

→ CNN에는 가로 세로 해상도 외에 **Channel (채널)** 이라는 차원을 추가한다. 보통 흑백 이미지에는 채널 하나, 컬러 이미지에는 채널 세개를 할당 (여기서 3개인 이유는 RGB?)

- 가로 해상도, 세로 해상도 그리고 Channel (세 번째 차원)
- 흑백 이미지: 모노 색상 채널 ⇒ [가로 해상도, 세로 해상도, 1]
- 컬러 이미지: RGB 삼원색 채널 ⇒ [가로 해상도, 세로 해상도, 3]
- 입력 채널 수: 최초 입력 혹은 이전 단계 출력 형태를 따름
- 출력 채널 수: 신경망 설계자가 정함

합성곱 계층의 출력 채널 수를 입력 채널 수보다 늘려 합성곱 계층을 거칠 때마다 정보량이 증가 된다. But, 합성곱 계층 사이에 배치된 풀링 계층으로 인해 정보량을 다시 줄여 준다.

→ 합성곱 계층이 채널 수를 2배 늘리면 정보량도 2배 증가 but, 이어지는 풀링 계층 stride가 2 x 2로 이미지 크기를 줄여준다면 정보량은 $\frac 14$, 이는 처음 정보량의 절반 (why? stride가 2 by 2라?)

해당 부분 조큼 어려워요 설명 가능 하신분?

### 입력 채널

- 커널은 해당 영역 모든 입력 채널의 픽셀값을 볼 수 있어야 함
- 하나의 feature map을 만들 커널은 [행, 열, 채널]의 3차원 형태가 필요

### 출력 채널

- 출력 채널 수 만큼의 특징맵을 만들기 위해서 그만큼의 커널이 필요
- 다수의 특징맵을 만들 커널은 [행, 열, 채널1, 채널2]의 4차원 형태가 필요

합성곱 계층의 처리는 [mini-batch size, 입력 이미지 행 수, 입력 이미지 열 수, 입력 채널 수] 형태의 4-dim 입력을 [커널 행 수, 커널 열 수, 입력 채널 수, 출력 채널 수] 형태의 4-dim 커널로 처리

→ [mini-batch size, 출력 이미지 행 수, 출력 이미지 열 수, 출력 채널 수] 형태의 4-dim 출력 도출

- 입력 형태: [mb, xh, xw, xchn]
- 커널 형태: [kh, kw, xchn, ychn]
- 출력 형태: [mb, yh, yw, ychn]
- SAME padding 방식과 건너뛰기 처리가 없음을 가정,
    - yh = xh, yw = xw가 되며 출력은 [mb, xh, xw, ychn]의 형태가 된다
        
        ![Screen Shot 2021-11-07 at 6.18.32 PM.png](CNN%20(Chapter%207%205~)%20c2a684f4ab2546d49f9d5c2511cc1612/Screen_Shot_2021-11-07_at_6.18.32_PM.png)
        
        ![Screen Shot 2021-11-07 at 6.31.48 PM.png](CNN%20(Chapter%207%205~)%20c2a684f4ab2546d49f9d5c2511cc1612/Screen_Shot_2021-11-07_at_6.31.48_PM.png)
        

채널 추가에 따른 풀링 계층

- 여러 채널의 특징맵들이 표현하는 서로 다른 도메인의 특징값들의 평균이나 최대치 도출은 직관적으로 유용하지 않다
- 특별히 성과를 거둔 연구 또한 없음
    
    → 풀링 계층에 채널 압축 기법은 거의 적용되지 않음. 풀링에서의 대푯값 간출은 개별 채널 안의 정만을 대상으로 함.
    
    → 풀링 층을 거쳐도 이미지 해상도가 감소 할 뿐, 채널 수는 변하지 않음
    

## 7.6 합성곱과 풀링의 Backpropagation

### Convolution layer's backpropagation

- 원리상 완전 연결 계층에서의 역전파 방법과 비슷하다
    - 입력 픽셀과 커널 가중치를 짝지어 곱하기 때문에 입력 픽셀과 커널 가중치는 서로가 서로의 계수로서 편미분값
    - 완전 연결 계층에서 입력과 가중치 사이의 관계와 같음
- 합성곱 연산은 4차원 입력과 4차원 커널로부터 4차원 출력을 도출하는 과정
    - 입력 픽셀과 커널 가중치의 손실 기울기는 영향을 미친 모든 출력 성분으로부터 부분적인 손실 기울기를 구해 합산해야 한다
    - 완전 연결 계층에서도 행렬 연산을 통해 하나의 입력 성분은 다수의 가중치에 곱해지며, 가중치 역시 다수의 입력 성분에 곱해진다. 따라서 역전파 과정에서는 영향을 미친 모든 출력 성분으로부터 손실 기울기를 구해 합사하는 과정이 숨겨진 채 실행된다
    - 순전파: $\Sigma$ 입력 x 가충치 = 출력
    - 역전파: $\Sigma$ 출력손실기울기 x 입력 = 가중치손실기울기
        
        역전파: $\Sigma$ 출력손실기울기 x 가중치 = 입력손실기울기
        

위 합성곱 계층 처리식을 역으로 정리하면 손실 기울기 계산을 다음과 같은 두 식으로 표현 됨:

![Screen Shot 2021-11-07 at 6.58.42 PM.png](CNN%20(Chapter%207%205~)%20c2a684f4ab2546d49f9d5c2511cc1612/Screen_Shot_2021-11-07_at_6.58.42_PM.png)

위 식에서 $dx, dy, dk$는 각각 $x, y, k$ 성분의 손실 기울기를 의미

*자세한 연산 방식은 7.8절 참조*

### Pooling layer's backpropagation

- 풀링 계층은 파라미터를 보유하지 않는다
- 출력 손실 기울기로부터 입력 손실 기울기를 어떻게 도출 하는지 보면 된다
- **Max Pooling**
    - max pooling에서 출력에 반영되는 것은 커널 영역의 최댓값 뿐
        
        → 역전파 처리 과정에서 최대치로 선정된 위치의 입력에만 출력의 손실 기울기를 전달, 나머지 입력의 손실 기울기를 0으로 지정
        
        → 풀링의 커널 크기와 건너뛰기 보폭이 달라 입력의 한 원소가 여러 구역에서 동시에 최댓값으로 선정된다면, 해당 출력들의 손실 기울기를 모두 전달받아 합산
        
        max pooling의 동점 처리 문제:
        
        - 한 구역의 최댓값이 입력 영역의 여러 원소에 동시에 나타났을 때의 처리 방법
            1. 그중 하나를 임의로 정해 출력의 손실 기울기를 그 원소에만 몰아준다 (가장 간단해 널리 이용됨)
            2. 동점인 모든 원소에 출력의 손실 기울기를 중복해 전달한다
            3. 출력의 손실 기울기를 동점인 입력 원소들에 균등하게 나누어준다
- **Average Pooling**
    - 모든 원소가 출력에 균등하게 반영된다
    - 출력의 손실 기울기를 입력 원소들에 균등하게 나누어주면 된다
        
        → 커널 영역의 크기를 $N$이라 할 때, 순전파 과정에서 평균 계산은 각 원소에 $\frac 1N$을 곱해 합산한다. 따라서 $\frac 1N$값이 모든 원소에 대해 계수로서 편미분 값이 된다
        
        → 역전파 과정에서도 출력의 손실 기울기에 $\frac 1N$값을 곱해 각 입력 성분의 손실 기울기로 전달하면 된다
        
        → 풀링의 커널 크기와 건너뛰기 보폭이 달라 한 원소가 여러 구역에서 평균값 계산에 이용된다면 각 구역에서 입력 손실 기울기를 따져 합산하면 된다
        
        average pooling에서의 경계 근처 처리 문제:
        
        - 이미지 크기가 커널 크기 및 보폭의 배수가 아닌 경우, 경계 근처의 풀링 계산이 중앙 부분과 다른 수의 입력 픽셀을 가지고 계산 되었다면 평균치를 계산할 때 나누어주는 숫자는 달라져야 한다
        - 출력 픽셀 위치별로 몇 개의 입력 픽셀이 평균값 계산에 이용되는지는 순전파, 역전파 모두에 필요
        - 해당 정보는 데이터에 따라 달라지는 것이 아닌 입력 형태, 풀링 계층의 커널 크기, 보폭에 따라 결정되므로 풀링 계층이 처음 만들어질 때 한 번만 구해 반복 활용하는 것이 효율적

합성곱 계층이나 풀링 계층의 처리 말미에 건너뛰기 처리가 있다면 역전파에서는 이에 대한 역처리가 먼저 수행되어야 한다

- 건너뛰기를 통해 선택된 입력에는 해당 위치 출력 성분의 손실기울기를 전달
- 탈락된 입력 성분에 대해서는 손실 기울기를 0으로 처리

## 7.7 CNN의 구성

CNN은 보통 convolution layer와 pooling layer를 교대로 배치하는 형태로 구성

- **Convolution layer**는 이미지의 해상도를 유지하면서 채널 수를 늘리는 역할 수행
- **Pooling layer**는 채널 수를 유지하면서 이미지 해상도를 줄이는 역할
    
    ![Screen Shot 2021-11-07 at 7.30.21 PM.png](CNN%20(Chapter%207%205~)%20c2a684f4ab2546d49f9d5c2511cc1612/Screen_Shot_2021-11-07_at_7.30.21_PM.png)
    

CNN과 FCP

- 완전 연결 계층의 출력은 이미지의 지역적 특성 표현이 불가
    - 완전 연결 계층 뒤에 합성곱 계층이나 풀링 계층 배치는 무의미
    - 완전 연결 계층은 합성곱이나 풀링 계층들 뒤에 배치하는 것이 보통
    - 완전 연결 계층 후에 합성곱이나 풀링 계층을 두는 것은 개념적으로 어울리지 않고 기술적으로도 번거로움

신경망 뒷단의 완전 연결 계층

- 지역적 특성 추출 후, 여러 채널에 흩어져 있는 정보를 종합하여 필요한 형태의 출력을 생성하기 위해서는 신경망 뒷단에 완전 연결 계층이 필수적으로 필요
- 출력 계층: MLP의 마지막 단계, 완전 연결 방식
    - 합성곱 신경망에서도 출력 계층을 그대로 활용
- 일부 큰 규모의 CNN의 경우, 출력 계층 앞 단계에 완전 출력 방식의 은닉 계층을 추가로 배치하는 접근 방식을 택하기도 하지만 널리 사용되진 않음